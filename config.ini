[GPT]
CONTEXT_SIZE = 1024
VOCAB_SIZE = 50304
EMBEDDING_DIM = 768
NUM_HEADS = 12
NUM_TRANSFORMER_BLOCK = 12


[TRAIN]
DATASET_PATH = data
WARMUP_STEPS = 2000
MAX_STEPS = 40000
BATCH_SIZE = 8
TOTAL_BATCH_SIZE = 524288
MAX_LEARNING_RATE = 6e-4
MIN_LEARNING_RATE = 6e-5
ADAM_BETA1 = 0.9
ADAM_BETA2 = 0.95
ADAM_EPS = 1e-8
WEIGHT_DECAY = 0.1
LOAD_WEIGHTS = false
SAVED_WEIGHTS_PATH = checkpoints
SAVE_WEIGHTS_FREQ = 500

[EVAL]
STEPS = 250
NUM_SEQUENCES = 4
MAX_LENGTH = 32
PROMPT = Hello, I am a language model,
TOP_K = 50
USE_COMPILE = false

[LOGGING]
LOCAL_LOG_FILE_NAME = logs/debug.log
PROJECT = GPT-2 from scratch
NAME =  Training from scratch
ARCH = GPT-2
DATASET = OpenWebText